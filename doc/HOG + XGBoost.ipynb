{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_extract_hog(path, hog, print_intvl=1000):\n",
    "    filenames = []\n",
    "    X = []\n",
    "    count = 0\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('jpg'):\n",
    "            filepath = os.path.join(path, file)\n",
    "            img = cv2.imread(filepath)\n",
    "            img_resize = cv2.resize(img, hog.winSize)\n",
    "            hog_values = hog.compute(img_resize).reshape((1, -1))\n",
    "            filename = file[:-4]\n",
    "            filenames.append(filename)\n",
    "            X.append(hog_values)\n",
    "            count += 1\n",
    "            if count % print_intvl == 0: print(count, end=' ')\n",
    "    print()\n",
    "    return (filenames, np.concatenate(X, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 \n",
      "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 \n",
      "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 \n"
     ]
    }
   ],
   "source": [
    "winSize = (64, 64)\n",
    "blockSize = (16, 16)\n",
    "blockStride = (8, 8)\n",
    "cellSize = (8, 8)\n",
    "nbins = 9\n",
    "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins)\n",
    "\n",
    "# Train\n",
    "train_cat_path = '../data/train/cat'\n",
    "train_dog_path = '../data/train/dog'\n",
    "_, X_train_cat = read_and_extract_hog(train_cat_path, hog)\n",
    "y_train_cat = np.zeros((X_train_cat.shape[0],))\n",
    "_, X_train_dog = read_and_extract_hog(train_dog_path, hog)\n",
    "y_train_dog = np.ones((X_train_dog.shape[0],))\n",
    "\n",
    "X_train = np.concatenate([X_train_cat, X_train_dog], axis=0)\n",
    "y_train = np.concatenate([y_train_cat, y_train_dog])\n",
    "\n",
    "# Test\n",
    "test_path = '../data/test'\n",
    "test_ids, X_test = read_and_extract_hog(test_path, hog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "[This tutorial](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) gives the following guidelines on how to select the best parameter:\n",
    "\n",
    "1. Start with a relatively high learning rate - 0.1 is usual, 0.05-0.3 is fine. The optimum number of trees for this learning rate will pop up as we run the cv function\n",
    "2. Tune tree-specific parameters (max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees)\n",
    "3. Tune regularization parameters (lambda, alpha)\n",
    "4. Lower the learning rate and decide the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find optimum number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "def modelfit(alg, X, y, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X, label=y)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, \n",
    "                          num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                          metrics=['auc', 'logloss'], early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X, y, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(X)\n",
    "    train_predprob = alg.predict_proba(X)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y, train_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, train_predprob))\n",
    "    print(\"Log Loss : %f\" % metrics.log_loss(y, train_predprob))\n",
    "                    \n",
    "    #feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    return alg\n",
    "    #feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    #plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb1 = XGBClassifier(\n",
    "    learning_rate=0.3, # Usual starting point\n",
    "    n_estimators=1000, # Usual starting point, will trim down later\n",
    "    max_depth=5,       # Should be between 3-10, 4-6 is fine\n",
    "    min_child_weight=1,# Default, will tune later to control overfitting\n",
    "    gamma=0,           # 0.1-0.2 is usually chosen to start, but 0 is fine for starting configuration\n",
    "    subsample=0.8,     # Commonly used start value, 0.5-0.9 is fine\n",
    "    colsample_bytree=0.8, # Same as above\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1, # Our dataset is not imbalanced, but leave default\n",
    "    seed=2018\n",
    ")\n",
    "alg = modelfit(xgb1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Report\n",
    "\n",
    "Accuracy : 0.9284\n",
    "\n",
    "AUC Score (Train): 0.979140\n",
    "\n",
    "Log Loss : 0.255223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=76,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=2018,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best n_estimators for learning rate 0.3: 76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best n_estimators for learning rate 0.1: 492"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find optimal max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test1 = {\n",
    "    'max_depth': range(3, 10, 2),\n",
    "    'min_child_weight': range(1, 6, 2)\n",
    "}\n",
    "\n",
    "xgb1 = XGBClassifier(\n",
    "    learning_rate=0.3, # Usual starting point\n",
    "    n_estimators=76, # Usual starting point, will trim down later\n",
    "    max_depth=5,       # Should be between 3-10, 4-6 is fine\n",
    "    min_child_weight=1,# Default, will tune later to control overfitting\n",
    "    gamma=0,           # 0.1-0.2 is usually chosen to start, but 0 is fine for starting configuration\n",
    "    subsample=0.8,     # Commonly used start value, 0.5-0.9 is fine\n",
    "    colsample_bytree=0.8, # Same as above\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1, # Our dataset is not imbalanced, but leave default\n",
    "    seed=2018\n",
    ")\n",
    "\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = xgb1,\n",
    "    param_grid = param_test1,\n",
    "    scoring = 'neg_log_loss',\n",
    "    n_jobs=-1,\n",
    "    iid=False,\n",
    "    cv=5\n",
    ")\n",
    "gsearch1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set and log loss found on development set:\n",
      "\n",
      "{'max_depth': 5, 'min_child_weight': 3} -0.5109441752548004\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-0.512 (+/-0.015) for {'max_depth': 3, 'min_child_weight': 1}\n",
      "-0.513 (+/-0.018) for {'max_depth': 3, 'min_child_weight': 3}\n",
      "-0.511 (+/-0.019) for {'max_depth': 3, 'min_child_weight': 5}\n",
      "-0.511 (+/-0.017) for {'max_depth': 5, 'min_child_weight': 1}\n",
      "-0.511 (+/-0.018) for {'max_depth': 5, 'min_child_weight': 3}\n",
      "-0.512 (+/-0.020) for {'max_depth': 5, 'min_child_weight': 5}\n",
      "-0.541 (+/-0.022) for {'max_depth': 7, 'min_child_weight': 1}\n",
      "-0.535 (+/-0.014) for {'max_depth': 7, 'min_child_weight': 3}\n",
      "-0.530 (+/-0.018) for {'max_depth': 7, 'min_child_weight': 5}\n",
      "-0.567 (+/-0.018) for {'max_depth': 9, 'min_child_weight': 1}\n",
      "-0.555 (+/-0.024) for {'max_depth': 9, 'min_child_weight': 3}\n",
      "-0.543 (+/-0.026) for {'max_depth': 9, 'min_child_weight': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set and log loss found on development set:\")\n",
    "print()\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = gsearch1.cv_results_['mean_test_score']\n",
    "stds = gsearch1.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gsearch1.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dig a bit deeper into finer range of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set and log loss found on development set:\n",
      "\n",
      "{'max_depth': 4, 'min_child_weight': 3} -0.5087565969602718\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-0.512 (+/-0.014) for {'max_depth': 4, 'min_child_weight': 2}\n",
      "-0.509 (+/-0.014) for {'max_depth': 4, 'min_child_weight': 3}\n",
      "-0.510 (+/-0.012) for {'max_depth': 4, 'min_child_weight': 4}\n",
      "-0.511 (+/-0.022) for {'max_depth': 5, 'min_child_weight': 2}\n",
      "-0.511 (+/-0.018) for {'max_depth': 5, 'min_child_weight': 3}\n",
      "-0.511 (+/-0.017) for {'max_depth': 5, 'min_child_weight': 4}\n",
      "-0.528 (+/-0.022) for {'max_depth': 6, 'min_child_weight': 2}\n",
      "-0.529 (+/-0.019) for {'max_depth': 6, 'min_child_weight': 3}\n",
      "-0.520 (+/-0.023) for {'max_depth': 6, 'min_child_weight': 4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[4,5,6],\n",
    " 'min_child_weight':[2,3,4]\n",
    "}\n",
    "\n",
    "xgb2 = XGBClassifier(\n",
    "    learning_rate=0.3, # Selected to speed up training\n",
    "    n_estimators=76, # Optimal for the chosen learning rate\n",
    "    max_depth=5,       # Should be between 3-10, 4-6 is fine\n",
    "    min_child_weight=3,# Default, will tune later to control overfitting\n",
    "    gamma=0,           # 0.1-0.2 is usually chosen to start, but 0 is fine for starting configuration\n",
    "    subsample=0.8,     # Commonly used start value, 0.5-0.9 is fine\n",
    "    colsample_bytree=0.8, # Same as above\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1, # Our dataset is not imbalanced, but leave default\n",
    "    seed=2018\n",
    ")\n",
    "\n",
    "gsearch2 = GridSearchCV(\n",
    "    estimator = xgb2,\n",
    "    param_grid = param_test2,\n",
    "    scoring = 'neg_log_loss',\n",
    "    n_jobs=-1,\n",
    "    iid=False,\n",
    "    cv=5\n",
    ")\n",
    "gsearch2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set and log loss found on development set:\")\n",
    "print()\n",
    "print(gsearch2.best_params_, gsearch2.best_score_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = gsearch2.cv_results_['mean_test_score']\n",
    "stds = gsearch2.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gsearch2.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tune gamma, based on parameters tuned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set and log loss found on development set:\n",
      "\n",
      "{'gamma': 0.4} -0.508565354911047\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-0.509 (+/-0.014) for {'gamma': 0.0}\n",
      "-0.509 (+/-0.013) for {'gamma': 0.1}\n",
      "-0.509 (+/-0.013) for {'gamma': 0.2}\n",
      "-0.509 (+/-0.013) for {'gamma': 0.3}\n",
      "-0.509 (+/-0.014) for {'gamma': 0.4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "xgb3 = XGBClassifier(\n",
    "    learning_rate=0.3, # Selected to speed up training\n",
    "    n_estimators=76, # Optimal for the chosen learning rate\n",
    "    max_depth=4,       # Should be between 3-10, 4-6 is fine\n",
    "    min_child_weight=3,# Default, will tune later to control overfitting\n",
    "    gamma=0,           # 0.1-0.2 is usually chosen to start, but 0 is fine for starting configuration\n",
    "    subsample=0.8,     # Commonly used start value, 0.5-0.9 is fine\n",
    "    colsample_bytree=0.8, # Same as above\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1, # Our dataset is not imbalanced, but leave default\n",
    "    seed=2018\n",
    ")\n",
    "\n",
    "gsearch3 = GridSearchCV(\n",
    "    estimator = xgb3,\n",
    "    param_grid = param_test3,\n",
    "    scoring = 'neg_log_loss',\n",
    "    n_jobs=-1,\n",
    "    iid=False,\n",
    "    cv=5\n",
    ")\n",
    "gsearch3.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set and log loss found on development set:\")\n",
    "print()\n",
    "print(gsearch3.best_params_, gsearch3.best_score_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = gsearch3.cv_results_['mean_test_score']\n",
    "stds = gsearch3.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gsearch3.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because 0.4 is at the tip of our range, we give it a further stretch to see if it improves further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set and log loss found on development set:\n",
      "\n",
      "{'gamma': 0.4} -0.508565354911047\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-0.509 (+/-0.014) for {'gamma': 0.4}\n",
      "-0.509 (+/-0.014) for {'gamma': 0.5}\n",
      "-0.509 (+/-0.014) for {'gamma': 0.6}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    "    'gamma': [0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(\n",
    "    estimator = xgb3,\n",
    "    param_grid = param_test4,\n",
    "    scoring = 'neg_log_loss',\n",
    "    n_jobs=-1,\n",
    "    iid=False,\n",
    "    cv=5\n",
    ")\n",
    "gsearch4.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set and log loss found on development set:\")\n",
    "print()\n",
    "print(gsearch4.best_params_, gsearch4.best_score_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = gsearch4.cv_results_['mean_test_score']\n",
    "stds = gsearch4.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gsearch4.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalibrate number of boosting rounds, which might have changed following previous tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9357\n",
      "AUC Score (Train): 0.983136\n",
      "Log Loss : 0.235981\n"
     ]
    }
   ],
   "source": [
    "xgb5 = XGBClassifier(\n",
    "    learning_rate=0.3, # Selected to speed up training\n",
    "    n_estimators=1000, # Usual starting point, will trim down after recalibration\n",
    "    max_depth=4,       # Chosen through grid search\n",
    "    min_child_weight=3,# Chosen through grid search\n",
    "    gamma=0.4,         # Chosen through grid search\n",
    "    subsample=0.8,     # Commonly used start value, 0.5-0.9 is fine\n",
    "    colsample_bytree=0.8, # Same as above\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1, # Our dataset is not imbalanced, but leave default\n",
    "    seed=2018\n",
    ")\n",
    "alg = modelfit(xgb5, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is possible to further tune *subsample* and *colsample_bytree*, the improvement will be marginal, so I stop here and begin to produce deliverables for Kaggle competition page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = alg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': yhat_test[:, 1]\n",
    "})\n",
    "df.to_csv('../output/predictions_hog_xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample CV to get a sense of actual accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "alg_clone = clone(alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 80.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9904, 2596],\n",
       "       [2640, 9860]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "xgb6 = XGBClassifier(\n",
    "    learning_rate=0.1, # Selected to speed up training\n",
    "    n_estimators=1000, # Usual starting point, will trim down after recalibration\n",
    "    max_depth=4,       # Chosen through grid search\n",
    "    min_child_weight=3,# Chosen through grid search\n",
    "    gamma=0.4,         # Chosen through grid search\n",
    "    subsample=0.8,     # Commonly used start value, 0.5-0.9 is fine\n",
    "    colsample_bytree=0.8, # Same as above\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1, # Our dataset is not imbalanced, but leave default\n",
    "    seed=2018\n",
    ")\n",
    "cv_predict = cross_val_predict(xgb6, X_train, y_train, n_jobs=-1, cv=5, verbose=1)\n",
    "confusion_matrix(y_train, cv_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79056"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train, cv_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
